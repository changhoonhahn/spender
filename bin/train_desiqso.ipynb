{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d049d93c-8f97-491c-9b93-70a206fa9167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/chahah/.conda/envs/gqp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from accelerate import Accelerator\n",
    "from spender import SpectrumAutoencoder\n",
    "from spender.data import desi_qso as desi \n",
    "from spender.util import mem_report, resample_to_restframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172c7ccb-0d22-41d8-84a4-60b6fb753d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train(seq,niter=800):\n",
    "    for d in seq:\n",
    "        if not \"iteration\" in d:d[\"iteration\"]=niter\n",
    "        if not \"encoder\" in d:d.update({\"encoder\":d[\"data\"]})\n",
    "    return seq\n",
    "\n",
    "def build_ladder(train_sequence):\n",
    "    n_iter = sum([item['iteration'] for item in train_sequence])\n",
    "\n",
    "    ladder = np.zeros(n_iter,dtype='int')\n",
    "    n_start = 0\n",
    "    for i,mode in enumerate(train_sequence):\n",
    "        n_end = n_start+mode['iteration']\n",
    "        ladder[n_start:n_end]= i\n",
    "        n_start = n_end\n",
    "    return ladder\n",
    "\n",
    "def get_all_parameters(models,instruments):\n",
    "    model_params = []\n",
    "    # multiple encoders\n",
    "    for model in models:\n",
    "        model_params += model.encoder.parameters()\n",
    "    # 1 decoder\n",
    "    model_params += model.decoder.parameters()\n",
    "    dicts = [{'params':model_params}]\n",
    "\n",
    "    n_parameters = sum([p.numel() for p in model_params if p.requires_grad])\n",
    "\n",
    "    instr_params = []\n",
    "    # instruments\n",
    "    for inst in instruments:\n",
    "        if inst==None:continue\n",
    "        instr_params += inst.parameters()\n",
    "        s = [p.numel() for p in inst.parameters()]\n",
    "    if instr_params != []:\n",
    "        dicts.append({'params':instr_params,'lr': 1e-4})\n",
    "        n_parameters += sum([p.numel() for p in instr_params if p.requires_grad])\n",
    "        print(\"parameter dict:\",dicts[1])\n",
    "    return dicts,n_parameters\n",
    "\n",
    "def consistency_loss(s, s_aug, individual=False):\n",
    "    batch_size, s_size = s.shape\n",
    "    x = torch.sum((s_aug - s)**2/(0.5)**2,dim=1)/s_size\n",
    "    sim_loss = torch.sigmoid(x)-0.5 # zero = perfect alignment\n",
    "    if individual:\n",
    "        return x, sim_loss\n",
    "    return sim_loss.sum()\n",
    "\n",
    "def similarity_loss(instrument, model, spec, w, z, s, slope=0.5, individual=False, wid=5, amp=3):\n",
    "    spec,w = resample_to_restframe(instrument.wave_obs,\n",
    "                                   model.decoder.wave_rest,\n",
    "                                   spec,w,z)\n",
    "\n",
    "    batch_size, spec_size = spec.shape\n",
    "    _, s_size = s.shape\n",
    "    device = s.device\n",
    "\n",
    "    # pairwise dissimilarity of spectra\n",
    "    S = (spec[None,:,:] - spec[:,None,:])**2\n",
    "\n",
    "    # pairwise weights\n",
    "    non_zero = w > 1e-6\n",
    "    N = (non_zero[None,:,:] * non_zero[:,None,:])\n",
    "    W = (1 / w)[None,:,:] + (1 / w)[:,None,:]\n",
    "    W =  N / W\n",
    "\n",
    "    N = N.sum(-1)\n",
    "    N[N==0] = 1\n",
    "    # dissimilarity of spectra\n",
    "    # of order unity, larger for spectrum pairs with more comparable bins\n",
    "    spec_sim = (W * S).sum(-1) / N\n",
    "\n",
    "    # dissimilarity of latents\n",
    "    s_sim = ((s[None,:,:] - s[:,None,:])**2).sum(-1) / s_size\n",
    "\n",
    "    # only give large loss of (dis)similarities are different (either way)\n",
    "    x = s_sim-spec_sim\n",
    "    sim_loss = torch.sigmoid(slope*x-0.5*wid)+torch.sigmoid(-slope*x-0.5*wid)\n",
    "    diag_mask = torch.diag(torch.ones(batch_size,device=device,dtype=bool))\n",
    "    sim_loss[diag_mask] = 0\n",
    "\n",
    "    if individual:\n",
    "        return s_sim,spec_sim,sim_loss\n",
    "    # total loss: sum over N^2 terms,\n",
    "    # needs to have amplitude of N terms to compare to fidelity loss\n",
    "    return amp*sim_loss.sum() / batch_size\n",
    "\n",
    "def restframe_weight(model,mu=5000,sigma=2000,amp=30):\n",
    "    x = model.decoder.wave_rest\n",
    "    return amp*torch.exp(-(0.5*(x-mu)/sigma)**2)\n",
    "\n",
    "def similarity_restframe(instrument, model, s=None, slope=1.0,\n",
    "                         individual=False, wid=5, bound=[4000,7000]):\n",
    "    _, s_size = s.shape\n",
    "    device = s.device\n",
    "\n",
    "    spec = model.decode(s)\n",
    "    wave = model.decoder.wave_rest\n",
    "    mask = (wave>bound[0])*(wave<bound[1])\n",
    "    spec /= spec[:,mask].median(dim=1)[0][:,None]\n",
    "    batch_size, spec_size = spec.shape\n",
    "    # pairwise dissimilarity of spectra\n",
    "    S = (spec[None,:,:] - spec[:,None,:])**2\n",
    "    # dissimilarity of spectra\n",
    "    # of order unity, larger for spectrum pairs with more comparable bins\n",
    "    W = restframe_weight(model)\n",
    "    spec_sim = (W * S).sum(-1) / spec_size\n",
    "    # dissimilarity of latents\n",
    "    s_sim = ((s[None,:,:] - s[:,None,:])**2).sum(-1) / s_size\n",
    "\n",
    "    # only give large loss of (dis)similarities are different (either way)\n",
    "    x = s_sim-spec_sim\n",
    "    sim_loss = torch.sigmoid(slope*x-wid/2)+torch.sigmoid(-slope*x-wid/2)\n",
    "    diag_mask = torch.diag(torch.ones(batch_size,device=device,dtype=bool))\n",
    "    sim_loss[diag_mask] = 0\n",
    "\n",
    "    if individual:\n",
    "        return s_sim,spec_sim,sim_loss\n",
    "\n",
    "    # total loss: sum over N^2 terms,\n",
    "    # needs to have amplitude of N terms to compare to fidelity loss\n",
    "    return sim_loss.sum() / batch_size\n",
    "\n",
    "def _losses(model,\n",
    "            instrument,\n",
    "            batch,\n",
    "            similarity=True,\n",
    "            slope=0,\n",
    "            skip=False\n",
    "           ):\n",
    "\n",
    "    spec, w, z = batch\n",
    "    # need the latents later on if similarity=True\n",
    "    s = model.encode(spec)\n",
    "    if skip: return 0,0,s\n",
    "    loss = model.loss(spec, w, instrument, z=z, s=s)\n",
    "\n",
    "    if similarity:\n",
    "        sim_loss = similarity_restframe(instrument, model, s, slope=slope)\n",
    "    else: sim_loss = 0\n",
    "\n",
    "    return loss, sim_loss, s\n",
    "\n",
    "def get_losses(model,\n",
    "               instrument,\n",
    "               batch,\n",
    "               aug_fct=None,\n",
    "               similarity=True,\n",
    "               consistency=True,\n",
    "               slope=0\n",
    "               ):\n",
    "\n",
    "    loss, sim_loss, s = _losses(model, instrument, batch, similarity=similarity, slope=slope)\n",
    "\n",
    "    if aug_fct is not None:\n",
    "        batch_copy = aug_fct(batch,z_max=args.z_max)\n",
    "        loss_, sim_loss_, s_ = _losses(model, instrument, batch_copy, similarity=similarity, slope=slope,skip=True)\n",
    "    else:\n",
    "        loss_ = sim_loss_ = 0\n",
    "\n",
    "    if consistency and aug_fct is not None:\n",
    "        cons_loss = slope*consistency_loss(s, s_)\n",
    "    else:\n",
    "        cons_loss = 0\n",
    "\n",
    "    return loss, sim_loss, loss_, sim_loss_, cons_loss\n",
    "\n",
    "\n",
    "def checkpoint(accelerator, args, optimizer, scheduler, n_encoder, outfile, losses):\n",
    "    unwrapped = [accelerator.unwrap_model(args_i).state_dict() for args_i in args]\n",
    "\n",
    "    accelerator.save({\n",
    "        \"model\": unwrapped,\n",
    "        \"losses\": losses,\n",
    "    }, outfile)\n",
    "    return\n",
    "\n",
    "def load_model(filename, models, instruments):\n",
    "    device = instruments[0].wave_obs.device\n",
    "    model_struct = torch.load(filename, map_location=device)\n",
    "    #wave_rest = model_struct['model'][0]['decoder.wave_rest']\n",
    "    for i, model in enumerate(models):\n",
    "        # backwards compat: encoder.mlp instead of encoder.mlp.mlp\n",
    "        if 'encoder.mlp.mlp.0.weight' in model_struct['model'][i].keys():\n",
    "            from collections import OrderedDict\n",
    "            model_struct['model'][i] = OrderedDict([(k.replace('mlp.mlp', 'mlp'), v) for k, v in model_struct['model'][i].items()])\n",
    "        # backwards compat: add instrument to encoder\n",
    "        try:\n",
    "            model.load_state_dict(model_struct['model'][i], strict=False)\n",
    "        except RuntimeError:\n",
    "            model_struct['model'][i]['encoder.instrument.wave_obs']= instruments[i].wave_obs\n",
    "            model_struct['model'][i]['encoder.instrument.skyline_mask']= instruments[i].skyline_mask\n",
    "            model.load_state_dict(model_struct[i]['model'], strict=False)\n",
    "\n",
    "    losses = model_struct['losses']\n",
    "    return models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01671e9c-b53a-4cc9-b958-a988ebc83188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restframe:\t720 .. 9824 A (9780 bins)\n",
      "/global/cfs/projectdirs/desi/users/chahah/spender_qso\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"dir\", help=\"data file directory\")\n",
    "# parser.add_argument(\"outfile\", help=\"output file name\")\n",
    "# parser.add_argument(\"-n\", \"--latents\", help=\"latent dimensionality\", type=int, default=2)\n",
    "# parser.add_argument(\"-b\", \"--batch_size\", help=\"batch size\", type=int, default=512)\n",
    "# parser.add_argument(\"-l\", \"--batch_number\", help=\"number of batches per epoch\", type=int, default=None)\n",
    "# parser.add_argument(\"-r\", \"--rate\", help=\"learning rate\", type=float, default=1e-3)\n",
    "# parser.add_argument(\"-zmax\", \"--z_max\", help=\"constrain redshifts to z_max\", type=float, default=0.8)\n",
    "# parser.add_argument(\"-a\", \"--augmentation\", help=\"add augmentation loss\", action=\"store_true\")\n",
    "# parser.add_argument(\"-s\", \"--similarity\", help=\"add similarity loss\", action=\"store_true\")\n",
    "# parser.add_argument(\"-c\", \"--consistency\", help=\"add consistency loss\", action=\"store_true\")\n",
    "# parser.add_argument(\"-C\", \"--clobber\", help=\"continue training of existing model\", action=\"store_true\")\n",
    "# parser.add_argument(\"-v\", \"--verbose\", help=\"verbose printing\", action=\"store_true\")\n",
    "z_max = 4.0\n",
    "_dir = '/global/cfs/projectdirs/desi/users/chahah/spender_qso'\n",
    "outfile = '/global/cfs/projectdirs/desi/users/chahah/spender_qso/models/model0.pt'\n",
    "batch_size = 128\n",
    "latents = 10 \n",
    "lr = 1e-3\n",
    "\n",
    "# define instruments\n",
    "instruments = [ desi.DESI() ]\n",
    "n_encoder = len(instruments)\n",
    "\n",
    "# restframe wavelength for reconstructed spectra\n",
    "# Note: represents joint dataset wavelength range\n",
    "lmbda_min = instruments[0].wave_obs[0]/(1.0+z_max) # 2000 A\n",
    "lmbda_max = instruments[0].wave_obs[-1] # 9824 A\n",
    "bins = 9780\n",
    "wave_rest = torch.linspace(lmbda_min, lmbda_max, bins, dtype=torch.float32)\n",
    "    \n",
    "print (\"Restframe:\\t{:.0f} .. {:.0f} A ({} bins)\".format(lmbda_min, lmbda_max, bins))\n",
    "\n",
    "print(_dir) \n",
    "\n",
    "# data loaders\n",
    "trainloaders = [ inst.get_data_loader(_dir, tag=\"qso0\", which=\"train\",  batch_size=batch_size, shuffle=True, shuffle_instance=True) for inst in instruments ]\n",
    "validloaders = [ inst.get_data_loader(_dir,  tag=\"qso0\", which=\"valid\", batch_size=batch_size, shuffle=True, shuffle_instance=True) for inst in instruments ]\n",
    "\n",
    "# get augmentation function\n",
    "#if args.augmentation:\n",
    "#    aug_fcts = [ desi.DESI().augment_spectra ]\n",
    "#else:\n",
    "aug_fcts = [ None ]\n",
    "\n",
    "# define training sequence\n",
    "FULL = {\"data\":[True],\"decoder\":True}\n",
    "train_sequence = prepare_train([FULL])\n",
    "\n",
    "annealing_step = 0.1\n",
    "ANNEAL_SCHEDULE = np.arange(0.0,2.0,annealing_step)\n",
    "\n",
    "#if args.verbose and args.similarity:\n",
    "#    print(\"similarity_slope:\",len(ANNEAL_SCHEDULE),ANNEAL_SCHEDULE)\n",
    "\n",
    "# define and train the model\n",
    "n_hidden = (64, 256, 1024)\n",
    "models = [ SpectrumAutoencoder(instrument,\n",
    "                               wave_rest,\n",
    "                               n_latent=latents,\n",
    "                               n_hidden=n_hidden,\n",
    "                               act=[nn.LeakyReLU()]*(len(n_hidden)+1)\n",
    "                               )\n",
    "          for instrument in instruments ]\n",
    "\n",
    "# use same decoder\n",
    "if n_encoder==2:models[1].decoder = models[0].decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930ee253-7975-4fbc-a16c-b9b88c564621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count(): 1\n",
      "--- Model /global/cfs/projectdirs/desi/users/chahah/spender_qso/models/model0.pt ---\n"
     ]
    }
   ],
   "source": [
    "n_epoch = sum([item['iteration'] for item in train_sequence])\n",
    "init_t = time.time()\n",
    "print(\"torch.cuda.device_count():\",torch.cuda.device_count())\n",
    "print (f\"--- Model {outfile} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1165a2b9-bc5d-4cfd-bce1-f4865ed74cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 13464190\n",
      "CPU RAM Free: 203.8 GB\n",
      "GPU 0 ... Mem Free: 19044MB / 40960MB | Utilization  52%\n"
     ]
    }
   ],
   "source": [
    "# def train(models,\n",
    "#           instruments,\n",
    "#           trainloaders,\n",
    "#           validloaders,\n",
    "#           n_epoch=200,\n",
    "#           outfile=None,\n",
    "#           losses=None,\n",
    "#           verbose=False,\n",
    "#           lr=1e-4,\n",
    "#           n_batch=50,\n",
    "#           aug_fcts=None,\n",
    "#           similarity=True,\n",
    "#           consistency=True,\n",
    "#           ):\n",
    "\n",
    "n_encoder = len(models)\n",
    "model_parameters, n_parameters = get_all_parameters(models,instruments)\n",
    "\n",
    "print(\"model parameters:\", n_parameters)\n",
    "mem_report()\n",
    "\n",
    "ladder = build_ladder(train_sequence)\n",
    "optimizer = torch.optim.Adam(model_parameters, lr=lr, eps=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr,\n",
    "                                          total_steps=n_epoch)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "models = [accelerator.prepare(model) for model in models]\n",
    "instruments = [accelerator.prepare(instrument) for instrument in instruments]\n",
    "trainloaders = [accelerator.prepare(loader) for loader in trainloaders]\n",
    "validloaders = [accelerator.prepare(loader) for loader in validloaders]\n",
    "optimizer = accelerator.prepare(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b61d2e7-516c-4a27-bcab-448e05313c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/c/chahah/.conda/envs/gqp/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "# define losses to track\n",
    "n_loss = 5\n",
    "epoch = 0\n",
    "detailed_loss = np.zeros((2, n_encoder, n_epoch, n_loss))\n",
    "\n",
    "for epoch_ in range(epoch, n_epoch):\n",
    "\n",
    "    mode = train_sequence[ladder[epoch_ - epoch]]\n",
    "\n",
    "    # turn on/off model decoder\n",
    "    for p in models[0].decoder.parameters():\n",
    "        p.requires_grad = mode['decoder']\n",
    "\n",
    "    slope = ANNEAL_SCHEDULE[(epoch_ - epoch)%len(ANNEAL_SCHEDULE)]\n",
    "    if n_epoch-epoch_<=10: slope=0 # turn off similarity\n",
    "\n",
    "    #if verbose and similarity:\n",
    "    #    print(\"similarity info:\",slope)\n",
    "\n",
    "    for which in range(n_encoder):\n",
    "\n",
    "        # turn on/off encoder\n",
    "        for p in models[which].encoder.parameters():\n",
    "            p.requires_grad = mode['encoder'][which]\n",
    "\n",
    "        # optional: training on single dataset\n",
    "        if not mode['data'][which]:\n",
    "            continue\n",
    "\n",
    "        models[which].train()\n",
    "        instruments[which].train()\n",
    "\n",
    "        n_sample = 0\n",
    "        for k, batch in enumerate(trainloaders[which]):\n",
    "            batch_size = len(batch[0])\n",
    "            print(batch_size)\n",
    "            losses = get_losses(\n",
    "                models[which],\n",
    "                instruments[which],\n",
    "                batch,\n",
    "                aug_fct=aug_fcts[which],\n",
    "                similarity=False, #similarity,\n",
    "                consistency=False, #consistency,\n",
    "                slope=slope,\n",
    "            )\n",
    "            # sum up all losses\n",
    "            loss = functools.reduce(lambda a, b: a+b , losses)\n",
    "            accelerator.backward(loss)\n",
    "            # clip gradients: stabilizes training with similarity\n",
    "            accelerator.clip_grad_norm_(model_parameters[0]['params'], 1.0)\n",
    "            # once per batch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # logging: training\n",
    "            detailed_loss[0][which][epoch_] += tuple( l.item() if hasattr(l, 'item') else 0 for l in losses )\n",
    "            n_sample += batch_size\n",
    "\n",
    "            # stop after n_batch\n",
    "            if n_batch is not None and k == n_batch - 1:\n",
    "                break\n",
    "        detailed_loss[0][which][epoch_] /= n_sample\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for which in range(n_encoder):\n",
    "            models[which].eval()\n",
    "            instruments[which].eval()\n",
    "\n",
    "            n_sample = 0\n",
    "            for k, batch in enumerate(validloaders[which]):\n",
    "                batch_size = len(batch[0])\n",
    "                losses = get_losses(\n",
    "                    models[which],\n",
    "                    instruments[which],\n",
    "                    batch,\n",
    "                    aug_fct=aug_fcts[which],\n",
    "                    similarity=False, #similarity,\n",
    "                    consistency=False, #consistency,\n",
    "                    slope=slope,\n",
    "                )\n",
    "                # logging: validation\n",
    "                detailed_loss[1][which][epoch_] += tuple( l.item() if hasattr(l, 'item') else 0 for l in losses )\n",
    "                n_sample += batch_size\n",
    "\n",
    "                # stop after n_batch\n",
    "                if n_batch is not None and k == n_batch - 1:\n",
    "                    break\n",
    "\n",
    "            detailed_loss[1][which][epoch_] /= n_sample\n",
    "\n",
    "    mem_report()\n",
    "    losses = tuple(detailed_loss[0, :, epoch_, :])\n",
    "    vlosses = tuple(detailed_loss[1, :, epoch_, :])\n",
    "    print('====> Epoch: %i'%(epoch))\n",
    "    print('TRAINING Losses:', losses)\n",
    "    print('VALIDATION Losses:', vlosses)\n",
    "\n",
    "    #if epoch_ % 5 == 0 or epoch_ == n_epoch - 1:\n",
    "    #    args = models\n",
    "    #    checkpoint(accelerator, args, optimizer, scheduler, n_encoder, outfile, detailed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364a4b1-5f59-4bc4-b377-2e0c19c09a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqp",
   "language": "python",
   "name": "gqp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
